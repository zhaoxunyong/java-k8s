   0.0 TEL | Telepresence 0.108 launched at Mon Dec 14 18:16:34 2020
   0.0 TEL |   /usr/bin/telepresence --new-deployment web-service --run-shell
   0.0 TEL | uname: uname_result(system='Linux', node='dave-PC', release='4.19.128-microsoft-standard', version='#1 SMP Tue Jun 23 12:58:10 UTC 2020', machine='x86_64', processor='x86_64')
   0.0 TEL | Platform: linux
   0.0 TEL | WSL: False
   0.0 TEL | Python 3.8.2 (default, Jul 16 2020, 14:00:26)
   0.0 TEL | [GCC 9.3.0]
   0.0 TEL | BEGIN SPAN main.py:40(main)
   0.0 TEL | BEGIN SPAN startup.py:83(set_kube_command)
   0.0 TEL | Found kubectl -> /usr/local/bin/kubectl
   0.1 TEL | [1] Capturing: kubectl config current-context
   0.2 TEL | [1] captured in 0.07 secs.
   0.2 TEL | [2] Capturing: kubectl --context docker-desktop version --short
   0.3 TEL | [2] captured in 0.10 secs.
   0.3 TEL | [3] Capturing: kubectl --context docker-desktop config view -o json
   0.3 TEL | [3] captured in 0.07 secs.
   0.3 TEL | [4] Capturing: kubectl --context docker-desktop api-versions
   0.4 TEL | [4] captured in 0.12 secs.
   0.4 TEL | Command: kubectl 1.19.3
   0.4 TEL | Context: docker-desktop, namespace: default, version: 1.19.3
   0.4 TEL | Looks like we're in a local VM, e.g. minikube.
   0.4 TEL | END SPAN startup.py:83(set_kube_command)    0.4s
   0.4 >>> | Using a Pod instead of a Deployment for the Telepresence proxy. If you experience problems, please file an issue!
   0.5 >>> | Set the environment variable TELEPRESENCE_USE_DEPLOYMENT to any non-empty value to force the old behavior, e.g.,
   0.5 >>> |     env TELEPRESENCE_USE_DEPLOYMENT=1 telepresence --run curl hello
   0.5 >>> | 
   0.5 TEL | Found ssh -> /usr/bin/ssh
   0.5 TEL | [5] Capturing: ssh -V
   0.5 TEL | [5] captured in 0.00 secs.
   0.5 TEL | Found bash -> /usr/bin/bash
   0.5 TEL | Found sshuttle-telepresence -> /usr/libexec/sshuttle-telepresence
   0.5 TEL | Found conntrack -> /usr/sbin/conntrack
   0.5 TEL | Found iptables -> /usr/sbin/iptables
   0.5 TEL | Found sudo -> /usr/bin/sudo
   0.5 TEL | [6] Running: sudo -n echo -n
   0.5 TEL | [6] ran in 0.01 secs.
   0.5 TEL | [7] Capturing: sudo iptables --list
   0.5 TEL | [7] captured in 0.01 secs.
   0.5 >>> | Starting proxy with method 'vpn-tcp', which has the following limitations: All processes are affected, only one telepresence can run per machine, and you can't use other VPNs. You may need to add cloud hosts and headless services with --also-proxy. For a full list of method limitations see https://telepresence.io/reference/methods.html
   0.5 TEL | Found sshfs -> /usr/bin/sshfs
   0.5 TEL | Found fusermount -> /usr/bin/fusermount
   0.5 >>> | Volumes are rooted at $TELEPRESENCE_ROOT. See https://telepresence.io/howto/volumes.html for details.
   0.5 TEL | [8] Running: kubectl --context docker-desktop --namespace default get pods telepresence-connectivity-check --ignore-not-found
   0.6 TEL | [8] ran in 0.10 secs.
   3.4 TEL | Scout info: {'latest_version': '0.108', 'application': 'telepresence', 'notices': []}
   3.4 >>> | Starting network proxy to cluster using new Pod web-service
   3.4 TEL | [9] Running: kubectl --context docker-desktop --namespace default create -f -
   3.7   9 | pod/web-service created
   3.7 TEL | [9] ran in 0.34 secs.
   3.7 TEL | BEGIN SPAN remote.py:109(wait_for_pod)
   3.7 TEL | [10] Running: kubectl --context docker-desktop --namespace default wait --for=condition=ready --timeout=60s pod/web-service
   5.2  10 | pod/web-service condition met
   5.2 TEL | [10] ran in 1.51 secs.
   5.2 TEL | [11] Capturing: kubectl --context docker-desktop --namespace default get pod web-service -o json
   5.4 TEL | [11] captured in 0.20 secs.
   5.4 TEL | END SPAN remote.py:109(wait_for_pod)    1.7s
   5.4 TEL | BEGIN SPAN connect.py:37(connect)
   5.4 TEL | [12] Launching kubectl logs: kubectl --context docker-desktop --namespace default logs -f web-service --container telepresence --tail=10
   5.4 TEL | [13] Launching kubectl port-forward: kubectl --context docker-desktop --namespace default port-forward web-service 41121:8022
   5.5 TEL | [14] Running: ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -oConnectTimeout=5 -q -p 41121 telepresence@127.0.0.1 /bin/true
   5.5 TEL | [14] exit 255 in 0.02 secs.
   5.7 TEL | [15] Running: ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -oConnectTimeout=5 -q -p 41121 telepresence@127.0.0.1 /bin/true
   5.8 TEL | [15] exit 255 in 0.04 secs.
   6.0  13 | Forwarding from 127.0.0.1:41121 -> 8022
   6.0  13 | Forwarding from [::1]:41121 -> 8022
   6.1 TEL | [16] Running: ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -oConnectTimeout=5 -q -p 41121 telepresence@127.0.0.1 /bin/true
   6.1  13 | Handling connection for 41121
   6.3 TEL | [16] ran in 0.22 secs.
   6.3 >>> | 
   6.3 >>> | No traffic is being forwarded from the remote Deployment to your local machine. You can use the --expose option to specify which ports you want to forward.
   6.3 >>> | 
   6.3 TEL | Launching Web server for proxy poll
   6.3 TEL | [17] Launching SSH port forward (socks and proxy poll): ssh -N -oServerAliveInterval=1 -oServerAliveCountMax=10 -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -oConnectTimeout=5 -q -p 41121 telepresence@127.0.0.1 -L127.0.0.1:46181:127.0.0.1:9050 -R9055:127.0.0.1:36963
   6.3 TEL | END SPAN connect.py:37(connect)    0.9s
   6.3 TEL | BEGIN SPAN remote_env.py:29(get_remote_env)
   6.3 TEL | [18] Capturing: kubectl --context docker-desktop --namespace default exec web-service --container telepresence -- python3 podinfo.py
   6.3  13 | Handling connection for 41121
   6.7 TEL | [18] captured in 0.42 secs.
   6.8 TEL | END SPAN remote_env.py:29(get_remote_env)    0.4s
   6.8 TEL | BEGIN SPAN mount.py:30(mount_remote_volumes)
   6.8 TEL | [19] Running: sshfs -p 41121 -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -oConnectTimeout=5 telepresence@127.0.0.1:/ /tmp/tel-co_8pcrh/fs
   6.8  13 | Handling connection for 41121
   6.8  12 | Retrieving this pod's namespace from the process environment
   6.8  12 | Failed: TELEPRESENCE_CONTAINER_NAMESPACE not set
   6.8  12 | Reading this pod's namespace from the k8s service account
   6.8  12 | Pod's namespace is 'default'
   6.8  12 | Listening...
   6.8  12 | 2020-12-14T10:16:40+0000 [-] Loading ./forwarder.py...
   6.8  12 | 2020-12-14T10:16:40+0000 [-] SOCKSv5Factory starting on 9050
   6.8  12 | 2020-12-14T10:16:40+0000 [socks.SOCKSv5Factory#info] Starting factory <socks.SOCKSv5Factory object at 0x7f940e5d4828>
   6.8  12 | 2020-12-14T10:16:40+0000 [-] DNSDatagramProtocol starting on 9053
   6.8  12 | 2020-12-14T10:16:40+0000 [-] Starting protocol <twisted.names.dns.DNSDatagramProtocol object at 0x7f940e5d4a20>
   6.8  12 | 2020-12-14T10:16:40+0000 [-] Loaded.
   6.8  12 | 2020-12-14T10:16:40+0000 [twisted.scripts._twistd_unix.UnixAppLogger#info] twistd 20.3.0 (/usr/bin/python3.6 3.6.8) starting up.
   6.8  12 | 2020-12-14T10:16:40+0000 [twisted.scripts._twistd_unix.UnixAppLogger#info] reactor class: twisted.internet.epollreactor.EPollReactor.
   7.0 TEL | [19] ran in 0.25 secs.
   7.0 TEL | END SPAN mount.py:30(mount_remote_volumes)    0.3s
   7.0 TEL | BEGIN SPAN vpn.py:62(connect_sshuttle)
   7.0 TEL | BEGIN SPAN cidr.py:113(get_proxy_cidrs)
   7.0 TEL | END SPAN cidr.py:113(get_proxy_cidrs)    0.0s
   7.0 TEL | [20] Launching sshuttle: sshuttle-telepresence -v --dns --method nat -e 'ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -oConnectTimeout=5' -r telepresence@127.0.0.1:41121 --to-ns 127.0.0.1:9053 10.1.0.0/24 192.168.65.0/24 10.96.0.0/12
   7.0 TEL | BEGIN SPAN vpn.py:85(connect_sshuttle,sshuttle-wait)
   7.0 TEL | Wait for vpn-tcp connection: hellotelepresence-0
   7.0 TEL | [21] Capturing: python3 -c 'import socket; socket.gethostbyname("hellotelepresence-0")'
   7.7  20 | Starting sshuttle proxy.
   8.4  20 | firewall manager: Starting firewall with Python version 3.8.2
   8.4  20 | firewall manager: ready method name nat.
   8.4  20 | IPv6 enabled: False
   8.4  20 | UDP enabled: False
   8.4  20 | DNS enabled: True
   8.4  20 | TCP redirector listening on ('127.0.0.1', 12300).
   8.4  20 | DNS listening on ('127.0.0.1', 12300).
   8.4  20 | Starting client with Python version 3.8.2
   8.4  20 | c : connecting to server...
   8.4  13 | Handling connection for 41121
   8.5  20 | Warning: Permanently added '[127.0.0.1]:41121' (ECDSA) to the list of known hosts.
   8.7  20 | Starting server with Python version 3.6.8
   8.7  20 |  s: latency control setting = True
   8.7  20 |  s: available routes:
   8.7  20 |  s:   2/10.1.0.0/16
   8.7  20 | c : Connected.
   8.7  20 | firewall manager: setting up.
   8.7  20 | >> iptables -t nat -D OUTPUT -j sshuttle-12300
   8.7  20 | >> iptables -t nat -D PREROUTING -j sshuttle-12300
   8.7  20 | iptables: No chain/target/match by that name.
   8.7  20 | error: ['iptables', '-t', 'nat', '-D', 'PREROUTING', '-j', 'sshuttle-12300'] returned 1
   8.7  20 | >> iptables -t nat -F sshuttle-12300
   8.8  20 | >> iptables -t nat -X sshuttle-12300
   8.8  20 | >> iptables -t nat -N sshuttle-12300
   8.8  20 | >> iptables -t nat -F sshuttle-12300
   8.8  20 | >> iptables -t nat -I OUTPUT 1 -j sshuttle-12300
   8.8  20 | >> iptables -t nat -I PREROUTING 1 -j sshuttle-12300
   8.8  20 | >> iptables -t nat -A sshuttle-12300 -j RETURN --dest 127.0.0.1/32 -p tcp
   8.8  20 | >> iptables -t nat -A sshuttle-12300 -j REDIRECT --dest 10.1.0.0/24 -p tcp --to-ports 12300 -m ttl ! --ttl 42
   8.8  20 | >> iptables -t nat -A sshuttle-12300 -j REDIRECT --dest 192.168.65.0/24 -p tcp --to-ports 12300 -m ttl ! --ttl 42
   8.8  20 | >> iptables -t nat -A sshuttle-12300 -j REDIRECT --dest 10.96.0.0/12 -p tcp --to-ports 12300 -m ttl ! --ttl 42
   8.8  20 | >> iptables -t nat -A sshuttle-12300 -j REDIRECT --dest 172.22.208.1/32 -p udp --dport 53 --to-ports 12300 -m ttl ! --ttl 42
   8.8  20 | >> iptables -t nat -A sshuttle-12300 -j REDIRECT --dest 224.0.0.252/32 -p udp --dport 5355 --to-ports 12300 -m ttl ! --ttl 42
   8.8  20 | conntrack v1.4.4 (conntrack-tools): 0 flow entries have been deleted.
   9.6 TEL | [21] exit 1 in 2.63 secs.
   9.7 TEL | [22] Capturing: python3 -c 'import socket; socket.gethostbyname("hellotelepresence-0.a.sanity.check.telepresence.io")'
   9.7  20 | c : DNS request from ('172.22.208.79', 55436) to None: 68 bytes
   9.8  12 | 2020-12-14T10:16:43+0000 [stdout#info] Sanity check: b'hellotelepresence-0.a.sanity.check.telepresence.io'
  10.1 TEL | [22] exit 1 in 0.47 secs.
  10.2 TEL | Wait for vpn-tcp connection: hellotelepresence-1
  10.2 TEL | [23] Capturing: python3 -c 'import socket; socket.gethostbyname("hellotelepresence-1")'
  10.3  20 | c : DNS request from ('172.22.208.79', 41098) to None: 37 bytes
  10.4  12 | 2020-12-14T10:16:44+0000 [stdout#info] Set DNS suffix we filter out to: [()]
  10.4  12 | 2020-12-14T10:16:44+0000 [stdout#info] Result for b'hellotelepresence-1' is ['127.0.0.1']
  10.4 TEL | [23] captured in 0.17 secs.
  10.4 TEL | Resolved hellotelepresence-1. 2 more...
  10.4 TEL | [24] Capturing: python3 -c 'import socket; socket.gethostbyname("hellotelepresence-1.a.sanity.check.telepresence.io")'
  10.5  20 | c : DNS request from ('172.22.208.79', 39615) to None: 68 bytes
  10.5  12 | 2020-12-14T10:16:44+0000 [stdout#info] Sanity check: b'hellotelepresence-1.a.sanity.check.telepresence.io'
  10.7 TEL | [24] exit 1 in 0.32 secs.
  10.8 TEL | Wait for vpn-tcp connection: hellotelepresence-2
  10.8 TEL | [25] Capturing: python3 -c 'import socket; socket.gethostbyname("hellotelepresence-2")'
  10.9  20 | c : DNS request from ('172.22.208.79', 35937) to None: 37 bytes
  10.9  12 | 2020-12-14T10:16:44+0000 [stdout#info] Result for b'hellotelepresence-2' is ['127.0.0.1']
  10.9 TEL | [25] captured in 0.12 secs.
  10.9 TEL | Resolved hellotelepresence-2. 1 more...
  10.9 TEL | [26] Capturing: python3 -c 'import socket; socket.gethostbyname("hellotelepresence-2.a.sanity.check.telepresence.io")'
  11.0  20 | c : DNS request from ('172.22.208.79', 37090) to None: 68 bytes
  11.0  12 | 2020-12-14T10:16:45+0000 [stdout#info] Sanity check: b'hellotelepresence-2.a.sanity.check.telepresence.io'
  11.3 TEL | [26] exit 1 in 0.32 secs.
  11.4 TEL | Wait for vpn-tcp connection: hellotelepresence-3
  11.4 TEL | [27] Capturing: python3 -c 'import socket; socket.gethostbyname("hellotelepresence-3")'
  11.4  20 | c : DNS request from ('172.22.208.79', 33502) to None: 37 bytes
  11.4  12 | 2020-12-14T10:16:45+0000 [stdout#info] Result for b'hellotelepresence-3' is ['127.0.0.1']
  11.5 TEL | [27] captured in 0.12 secs.
  11.5 TEL | Resolved hellotelepresence-3. 0 more...
  11.5 TEL | END SPAN vpn.py:85(connect_sshuttle,sshuttle-wait)    4.5s
  11.5 TEL | END SPAN vpn.py:62(connect_sshuttle)    4.5s
  11.5 >>> | Setup complete. Launching your command.
  11.5 TEL | Everything launched. Waiting to exit...
  11.5 TEL | BEGIN SPAN runner.py:729(wait_for_exit)
  30.5 TEL | [28] Running: sudo -n echo -n
  30.5 TEL | [28] ran in 0.01 secs.
  31.0  20 | c : DNS request from ('172.22.208.79', 43666) to None: 44 bytes
  31.0  12 | 2020-12-14T10:17:04+0000 [stdout#info] 12 query: b'79.208.22.172.in-addr.arpa'
  31.0  12 | 2020-12-14T10:17:04+0000 [DNSDatagramProtocol (UDP)] DNSDatagramProtocol starting on 6395
  31.0  12 | 2020-12-14T10:17:04+0000 [DNSDatagramProtocol (UDP)] Starting protocol <twisted.names.dns.DNSDatagramProtocol object at 0x7f940e5e82b0>
  31.0  12 | 2020-12-14T10:17:05+0000 [-] (UDP Port 6395 Closed)
  31.0  12 | 2020-12-14T10:17:05+0000 [-] Stopping protocol <twisted.names.dns.DNSDatagramProtocol object at 0x7f940e5e82b0>
  32.8  20 | c : DNS request from ('172.22.208.79', 50451) to None: 44 bytes
  32.8  12 | 2020-12-14T10:17:06+0000 [stdout#info] 12 query: b'79.208.22.172.in-addr.arpa'
  32.8  12 | 2020-12-14T10:17:06+0000 [DNSDatagramProtocol (UDP)] DNSDatagramProtocol starting on 52959
  32.8  12 | 2020-12-14T10:17:06+0000 [DNSDatagramProtocol (UDP)] Starting protocol <twisted.names.dns.DNSDatagramProtocol object at 0x7f940e5e8358>
  32.8  12 | 2020-12-14T10:17:06+0000 [-] (UDP Port 52959 Closed)
  32.8  12 | 2020-12-14T10:17:06+0000 [-] Stopping protocol <twisted.names.dns.DNSDatagramProtocol object at 0x7f940e5e8358>
  36.8 TEL | (proxy checking local liveness)
  36.8  12 | 2020-12-14T10:17:10+0000 [Poll#info] Checkpoint
  39.4  20 | c : DNS request from ('172.22.208.79', 50308) to None: 44 bytes
  39.4  12 | 2020-12-14T10:17:13+0000 [stdout#info] 12 query: b'79.208.22.172.in-addr.arpa'
  39.4  12 | 2020-12-14T10:17:13+0000 [DNSDatagramProtocol (UDP)] DNSDatagramProtocol starting on 30366
  39.4  12 | 2020-12-14T10:17:13+0000 [DNSDatagramProtocol (UDP)] Starting protocol <twisted.names.dns.DNSDatagramProtocol object at 0x7f940e6297f0>
  39.4  12 | 2020-12-14T10:17:13+0000 [-] (UDP Port 30366 Closed)
  39.4  12 | 2020-12-14T10:17:13+0000 [-] Stopping protocol <twisted.names.dns.DNSDatagramProtocol object at 0x7f940e6297f0>
  39.8  20 | c : DNS request from ('172.22.208.79', 38153) to None: 44 bytes
  39.8  12 | 2020-12-14T10:17:13+0000 [stdout#info] 12 query: b'79.208.22.172.in-addr.arpa'
  39.8  12 | 2020-12-14T10:17:13+0000 [DNSDatagramProtocol (UDP)] DNSDatagramProtocol starting on 42680
  39.8  12 | 2020-12-14T10:17:13+0000 [DNSDatagramProtocol (UDP)] Starting protocol <twisted.names.dns.DNSDatagramProtocol object at 0x7f940e6298d0>
  39.8  12 | 2020-12-14T10:17:13+0000 [-] (UDP Port 42680 Closed)
  39.8  12 | 2020-12-14T10:17:13+0000 [-] Stopping protocol <twisted.names.dns.DNSDatagramProtocol object at 0x7f940e6298d0>
  60.5 TEL | [29] Running: sudo -n echo -n
  60.6 TEL | [29] ran in 0.03 secs.
  66.8 TEL | (proxy checking local liveness)
  66.8  12 | 2020-12-14T10:17:40+0000 [Poll#info] Checkpoint
  90.6 TEL | [30] Running: sudo -n echo -n
  90.6 TEL | [30] ran in 0.03 secs.
  96.8 TEL | (proxy checking local liveness)
  96.8  12 | 2020-12-14T10:18:10+0000 [Poll#info] Checkpoint
 120.7 TEL | [31] Running: sudo -n echo -n
 120.7 TEL | [31] ran in 0.03 secs.
 126.8 TEL | (proxy checking local liveness)
 126.8  12 | 2020-12-14T10:18:40+0000 [Poll#info] Checkpoint
 150.7 TEL | [32] Running: sudo -n echo -n
 150.8 TEL | [32] ran in 0.01 secs.
 156.8 TEL | (proxy checking local liveness)
 156.8  12 | 2020-12-14T10:19:10+0000 [Poll#info] Checkpoint
 180.8 TEL | [33] Running: sudo -n echo -n
 180.8 TEL | [33] ran in 0.02 secs.
 186.8 TEL | (proxy checking local liveness)
 186.8  12 | 2020-12-14T10:19:40+0000 [Poll#info] Checkpoint
 210.8 TEL | [34] Running: sudo -n echo -n
 210.8 TEL | [34] ran in 0.01 secs.
 216.8 TEL | (proxy checking local liveness)
 216.8  12 | 2020-12-14T10:20:10+0000 [Poll#info] Checkpoint
 240.9 TEL | [35] Running: sudo -n echo -n
 240.9 TEL | [35] ran in 0.01 secs.
 246.8 TEL | (proxy checking local liveness)
 246.8  12 | 2020-12-14T10:20:40+0000 [Poll#info] Checkpoint
 270.9 TEL | [36] Running: sudo -n echo -n
 270.9 TEL | [36] ran in 0.01 secs.
 276.8 TEL | (proxy checking local liveness)
 276.8  12 | 2020-12-14T10:21:10+0000 [Poll#info] Checkpoint
 301.0 TEL | [37] Running: sudo -n echo -n
 301.0 TEL | [37] ran in 0.01 secs.
 306.8 TEL | (proxy checking local liveness)
 306.8  12 | 2020-12-14T10:21:40+0000 [Poll#info] Checkpoint
 331.0 TEL | [38] Running: sudo -n echo -n
 331.0 TEL | [38] ran in 0.02 secs.
 336.8 TEL | (proxy checking local liveness)
 336.8  12 | 2020-12-14T10:22:10+0000 [Poll#info] Checkpoint
 361.1 TEL | [39] Running: sudo -n echo -n
 361.1 TEL | [39] ran in 0.02 secs.
 366.8 TEL | (proxy checking local liveness)
 366.8  12 | 2020-12-14T10:22:40+0000 [Poll#info] Checkpoint
 391.1 TEL | [40] Running: sudo -n echo -n
 391.1 TEL | [40] ran in 0.03 secs.
 396.8 TEL | (proxy checking local liveness)
 396.8  12 | 2020-12-14T10:23:10+0000 [Poll#info] Checkpoint
 421.2 TEL | [41] Running: sudo -n echo -n
 421.2 TEL | [41] ran in 0.02 secs.
 426.8 TEL | (proxy checking local liveness)
 426.8  12 | 2020-12-14T10:23:40+0000 [Poll#info] Checkpoint
 448.5 TEL | Main process (bash --norc)
 448.5 TEL |  exited with code 0.
 448.6 TEL | END SPAN runner.py:729(wait_for_exit)  437.1s
 448.6 >>> | Your process has exited.
 448.6 TEL | EXITING successful session.
 448.6 >>> | Exit cleanup in progress
 448.6 TEL | (Cleanup) Terminate local process
 448.6 TEL | Local process is already dead (ret=0)
 448.6 TEL | (Cleanup) Kill BG process [20] sshuttle
 448.6 TEL | (Cleanup) Unmount remote filesystem
 448.6 TEL | [42] Running: fusermount -z -u /tmp/tel-co_8pcrh/fs
 448.6  20 | >> iptables -t nat -D OUTPUT -j sshuttle-12300
 448.6  20 | >> iptables -t nat -D PREROUTING -j sshuttle-12300
 448.6 TEL | [42] ran in 0.01 secs.
 448.6  20 | >> iptables -t nat -F sshuttle-12300
 448.6 TEL | (Cleanup) Kill BG process [17] SSH port forward (socks and proxy poll)
 448.6 TEL | [17] SSH port forward (socks and proxy poll): exit 0
 448.6 TEL | (Cleanup) Kill Web server for proxy poll
 448.6  20 | >> iptables -t nat -X sshuttle-12300
 448.7 TEL | [20] sshuttle: exit -15
 448.8 TEL | (Cleanup) Kill BG process [13] kubectl port-forward
 448.8 TEL | [13] kubectl port-forward: exit -15
 448.8 TEL | (Cleanup) Kill BG process [12] kubectl logs
 448.8 TEL | [12] kubectl logs: exit -15
 448.8 TEL | Background process (kubectl logs) exited with return code -15. Command was:
 448.8 TEL | (Cleanup) Delete proxy Pod
 448.8 >>> | Cleaning up Pod
 448.8 TEL |   kubectl --context docker-desktop --namespace default logs -f web-service --container telepresence --tail=10
 448.8 TEL | 
 448.8 TEL | [43] Running: kubectl --context docker-desktop --namespace default delete --ignore-not-found --wait=false --selector=telepresence=fb5e660d644b415ebc0d585341400c76 Pod
 448.8 TEL | Recent output was:
 448.8 TEL |   2020-12-14T10:19:10+0000 [Poll#info] Checkpoint
 448.8 TEL |   2020-12-14T10:19:40+0000 [Poll#info] Checkpoint
 448.8 TEL |   2020-12-14T10:20:10+0000 [Poll#info] Checkpoint
 448.8 TEL |   2020-12-14T10:20:40+0000 [Poll#info] Checkpoint
 448.8 TEL |   2020-12-14T10:21:10+0000 [Poll#info] Checkpoint
 448.8 TEL |   2020-12-14T10:21:40+0000 [Poll#info] Checkpoint
 448.8 TEL |   2020-12-14T10:22:10+0000 [Poll#info] Checkpoint
 448.8 TEL |   2020-12-14T10:22:40+0000 [Poll#info] Checkpoint
 448.8 TEL |   2020-12-14T10:23:10+0000 [Poll#info] Checkpoint
 448.8 TEL |   2020-12-14T10:23:40+0000 [Poll#info] Checkpoint
 449.8  43 | pod "web-service" deleted
 449.8 TEL | [43] ran in 1.00 secs.
 449.8 TEL | (Cleanup) Kill sudo privileges holder
 449.8 TEL | (Cleanup) Stop time tracking
 449.8 TEL | END SPAN main.py:40(main)  449.8s
 449.8 TEL | SPAN SUMMARY:
 449.8 TEL |  449.8s main.py:40(main)
 449.8 TEL |    0.4s   startup.py:83(set_kube_command)
 449.8 TEL |    0.1s     1 kubectl config current-context
 449.8 TEL |    0.1s     2 kubectl --context docker-desktop version --short
 449.8 TEL |    0.1s     3 kubectl --context docker-desktop config view -o json
 449.8 TEL |    0.1s     4 kubectl --context docker-desktop api-versions
 449.8 TEL |    0.0s   5 ssh -V
 449.8 TEL |    0.0s   6 sudo -n echo -n
 449.8 TEL |    0.0s   7 sudo iptables --list
 449.8 TEL |    0.1s   8 kubectl --context docker-desktop --namespace default get pods telepresence-con
 449.8 TEL |    0.3s   9 kubectl --context docker-desktop --namespace default create -f -
 449.8 TEL |    1.7s   remote.py:109(wait_for_pod)
 449.8 TEL |    1.5s     10 kubectl --context docker-desktop --namespace default wait --for=condition=rea
 449.8 TEL |    0.2s     11 kubectl --context docker-desktop --namespace default get pod web-service -o j
 449.8 TEL |    0.9s   connect.py:37(connect)
 449.8 TEL |    0.0s     14 ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -o
 449.8 TEL |    0.0s     15 ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -o
 449.8 TEL |    0.2s     16 ssh -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/dev/null -o
 449.8 TEL |    0.4s   remote_env.py:29(get_remote_env)
 449.8 TEL |    0.4s     18 kubectl --context docker-desktop --namespace default exec web-service --conta
 449.8 TEL |    0.3s   mount.py:30(mount_remote_volumes)
 449.8 TEL |    0.2s     19 sshfs -p 41121 -F /dev/null -oStrictHostKeyChecking=no -oUserKnownHostsFile=/
 449.9 TEL |    4.5s   vpn.py:62(connect_sshuttle)
 449.9 TEL |    0.0s     cidr.py:113(get_proxy_cidrs)
 449.9 TEL |    4.5s     vpn.py:85(connect_sshuttle,sshuttle-wait)
 449.9 TEL |    2.6s       21 python3 -c 'import socket; socket.gethostbyname("hellotelepresence-0")'
 449.9 TEL |    0.5s       22 python3 -c 'import socket; socket.gethostbyname("hellotelepresence-0.a.sanity
 449.9 TEL |    0.2s       23 python3 -c 'import socket; socket.gethostbyname("hellotelepresence-1")'
 449.9 TEL |    0.3s       24 python3 -c 'import socket; socket.gethostbyname("hellotelepresence-1.a.sanity
 449.9 TEL |    0.1s       25 python3 -c 'import socket; socket.gethostbyname("hellotelepresence-2")'
 449.9 TEL |    0.3s       26 python3 -c 'import socket; socket.gethostbyname("hellotelepresence-2.a.sanity
 449.9 TEL |    0.1s       27 python3 -c 'import socket; socket.gethostbyname("hellotelepresence-3")'
 449.9 TEL |  437.1s   runner.py:729(wait_for_exit)
 449.9 TEL |    0.0s     28 sudo -n echo -n
 449.9 TEL |    0.0s     29 sudo -n echo -n
 449.9 TEL |    0.0s     30 sudo -n echo -n
 449.9 TEL |    0.0s     31 sudo -n echo -n
 449.9 TEL |    0.0s     32 sudo -n echo -n
 449.9 TEL |    0.0s     33 sudo -n echo -n
 449.9 TEL |    0.0s     34 sudo -n echo -n
 449.9 TEL |    0.0s     35 sudo -n echo -n
 449.9 TEL |    0.0s     36 sudo -n echo -n
 449.9 TEL |    0.0s     37 sudo -n echo -n
 449.9 TEL |    0.0s     38 sudo -n echo -n
 449.9 TEL |    0.0s     39 sudo -n echo -n
 449.9 TEL |    0.0s     40 sudo -n echo -n
 449.9 TEL |    0.0s     41 sudo -n echo -n
 449.9 TEL |    0.0s   42 fusermount -z -u /tmp/tel-co_8pcrh/fs
 449.9 TEL |    1.0s   43 kubectl --context docker-desktop --namespace default delete --ignore-not-foun
 449.9 TEL | (Cleanup) Remove temporary directory
 449.9 TEL | (Cleanup) Save caches
 450.2 TEL | (sudo privileges holder thread exiting)
